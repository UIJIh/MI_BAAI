nohup: ignoring input
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
  0%|          | 0/15676 [00:00<?, ?it/s] 15%|█▌        | 2376/15676 [00:00<00:00, 23757.53it/s] 34%|███▍      | 5341/15676 [00:00<00:00, 27221.53it/s] 53%|█████▎    | 8293/15676 [00:00<00:00, 28268.95it/s] 71%|███████▏  | 11171/15676 [00:00<00:00, 28466.76it/s] 90%|████████▉ | 14105/15676 [00:00<00:00, 28780.29it/s]100%|██████████| 15676/15676 [00:00<00:00, 28321.37it/s]
/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '0.13.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
Vocab before resize: 128256
Vocab after resize: 128256
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: eyuansu62. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /share/project/gsai/uiji/translation/wandb/run-20241219_045837-4aenct0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ./model_output/llama3_idioms_1
wandb: ⭐️ View project at https://wandb.ai/eyuansu62/huggingface
wandb: 🚀 View run at https://wandb.ai/eyuansu62/huggingface/runs/4aenct0n
  0%|          | 0/63 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/share/project/gsai/uiji/translation/SFT.py", line 132, in <module>
    # TRAIN!!!
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/transformers/trainer.py", line 3579, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/transformers/trainer.py", line 3633, in compute_loss
    outputs = model(**inputs)
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1214, in forward
    loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **loss_kwargs)
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/transformers/loss/loss_utils.py", line 46, in ForCausalLMLoss
    loss = fixed_cross_entropy(shift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
  File "/root/anaconda3/envs/uiji_10.16/lib/python3.10/site-packages/transformers/loss/loss_utils.py", line 28, in fixed_cross_entropy
    loss = loss / num_items_in_batch
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:0!
[1;34mwandb[0m: 🚀 View run [33m./model_output/llama3_idioms_1[0m at: [34mhttps://wandb.ai/eyuansu62/huggingface/runs/4aenct0n[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241219_045837-4aenct0n/logs[0m
